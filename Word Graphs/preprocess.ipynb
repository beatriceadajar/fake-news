{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Documentation\n",
    "\n",
    "<insert why preprocessing is important here>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________________________________________________________________\n",
    "### Step 1: Import necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from TagalogStemmer import TglStemmer\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### spaCy\n",
    " \n",
    "The spaCy library is used for preprocessing and lemmatizing the English words. It has a built in English dictionary and is comparitively faster than NLTK in word tokenization.\n",
    " <br \\>\n",
    " \n",
    " #### TagalogStemmer\n",
    " Tagalog is currently not supported in NLP libraries in python we used the Tagalog Words Stemmer (https://github.com/crlwingen/TagalogStemmerPython/blob/master/TglStemmer.py) to process Tagalog words. It should removes the affixes of the Tagalog word and returns the root word. \n",
    " \n",
    " *Note: I modified it slightly by removing the print statements and cleaning the input.*\n",
    " <br \\>\n",
    " \n",
    " ___________________\n",
    " Modifications\n",
    " <ul>\n",
    " <li>\n",
    " *In reading from string:* \t\n",
    " From: \n",
    " ```python\n",
    " tokens = source.split(' ')\n",
    " ``` \n",
    " To: \n",
    " ```python\n",
    " tokens = source.strip().split(' ')\n",
    " ```\n",
    "    \n",
    " <li> \n",
    " *In reading from file:*\n",
    " \n",
    " From:\n",
    " ```python\n",
    " with open(source, 'r') as myfile:\n",
    " data = myfile.read().replace('\\n', ' ')\n",
    "    \n",
    " return data.split(' ')\n",
    " ```\n",
    " to:\n",
    " \n",
    " ```python\n",
    " with open(source, 'r', encoding='utf-8', errors='ignore') as myfile:\n",
    " data = myfile.read()\n",
    " data = re.sub(r'[^\\w]', ' ', data)\n",
    " data = re.sub(r'\\s+', ' ', data).strip()\n",
    " \n",
    " return data.split(' ')\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________________\n",
    "### Step 2: Read the JSON data \n",
    "Reads from the result of the fb-scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data.json') as json_data:\n",
    "    data = json.load(json_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br \\ >\n",
    "Sample data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Poor Old Man Accidentally Scratches Luxurious Car, Leaves Note That Touches Everyoneâ€™s Heart', 'created_time': '2017-12-24T20:29:19+0000', 'like': 52, 'love': 2, 'wow': 0, 'haha': 0, 'sad': 2, 'angry': 0, 'thankful': 0, 'total_reacts': 56, 'comments': 1, 'shares': 4, 'id': '979290192127575_1645818285474759', 'page_id': 'ClassifiedTrends'}\n"
     ]
    }
   ],
   "source": [
    "print(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________\n",
    "### Step 3: Clean the string\n",
    "For each message in the data, using the regex library: remove urls, special characters, extra white space and numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = []\n",
    "for d in data:\n",
    "\ttry:\n",
    "\t\tmessage = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', ' ', d['message'])\n",
    "\t\tmessage = re.sub(r'([^\\w]|[0-9_])+', ' ', message)\n",
    "\t\tmessage = re.sub(r'\\s+', ' ', message).strip()\n",
    "\t\tmessages.append(message)\n",
    "\texcept:\n",
    "\t\t#Skip if there is no message\n",
    "\t\tcontinue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample messages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sobrang nakakaawa itong sinapit ng batang ito na kung saan ay naipit ang kanyang kaliwang kamay sa escalator sa Gaisano Mall Panoorin nyo po ang buong pangyayari\n"
     ]
    }
   ],
   "source": [
    "print(messages[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top stories you might have missed Plan to get ship shape and competitive Philippine exporters are not only challenged by competition from traders in other Asean countries they are also challenged by the tired approach to shipping goods in their own All that may change though with the floating of the Philippine Export Development Plan PEDP a four pronged strategy devised by the Philippine Export Marketing Bureau EMB to put some wind in the sails of the country s flagging export sector\n"
     ]
    }
   ],
   "source": [
    "print(messages[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________________\n",
    "### Step 4: Consider Tagalog stop words\n",
    "Since the Tagalog Stemmer only returns the root of tagalog words, we have to add the Tagalog stop words to spaCy's dictionary.\n",
    "\n",
    "*Tagalog stop words are from https://github.com/stopwords-iso/stopwords-tl/blob/master/stopwords-tl.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load English Dictionary\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "#Add new words to dictionary as stop word\n",
    "tl_stop = ['lang', 'eto', 'kanila', 'de', 'nang', 'ito', 'di','silang', 'kanilang', 'at', 'rin', 'yan', 'pa', 'sa', 'mga', 'ay', 'din', 'na', 'ng', 'nag', 'mag', 'pag', 'ang', 'nya', 'nyo', 'sya', 'niyo', 'siya', 'kung', 'po', 'ito', 'iyan', 'akin','aking','ako','alin','am','amin','aming','ano','anumang','apat','at','atin','ating','ay','bababa','bago','bakit','bawat','bilang','dahil','dalawa','dapat','din','dito','doon','gagawin','gayunman','ginagawa','ginawa','ginawang','gumawa','gusto','habang','hanggang','hindi','huwag','iba','ibaba','ibabaw','ibig','ikaw','ilagay','ilalim','ilan','inyong','isa','isang','itaas','ito','iyo','iyon','iyong','ka','kahit','kailangan','kailanman','kami','kanila','kanilang','kanino','kanya','kanyang','kapag','kapwa','karamihan','katiyakan','katulad','kaya','kaysa','ko','kong','kulang','kumuha','kung','laban','lahat','lamang','likod','lima','maaari','maaaring','maging','mahusay','makita','marami','marapat','masyado','may','mayroon','mga','minsan','mismo','mula','muli','na','nabanggit','naging','nagkaroon','nais','nakita','namin','napaka','narito','nasaan','ng','ngayon','ni','nila','nilang','nito','niya','niyang','noon','o','pa','paano','pababa','paggawa','pagitan','pagkakaroon','pagkatapos','palabas','pamamagitan','panahon','pangalawa','para','paraan','pareho','pataas','pero','pumunta','pumupunta','sa','saan','sabi','sabihin','sarili','sila','sino','siya','tatlo','tayo','tulad','tungkol','una','walang']\n",
    "\n",
    "new_words = tl_stop\n",
    "for word in new_words:\n",
    "\tlexeme = nlp.vocab[word]\n",
    "\tlexeme.is_stop = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________\n",
    "\n",
    "### Step 5: Generate bag of words\n",
    "\n",
    "For each message divide into three separate lists because each list will be cleaned differently. This is to consider that the language may be Tagalog, Taglish, or English. *Other Filipino languages (e.g Bisaya) will be included in the tagalog list)*\n",
    "\n",
    "Since we will only be using bag of words, order does not matter.\n",
    "\n",
    "<br \\>\n",
    "\n",
    "Lists:\n",
    "<ul>\n",
    "    <li> ```english``` parsed using the standard lemma_ of spaCy's nlp. </li>\n",
    "    <li>```proper``` parses also using the standard lemma_of spaCy's nlp.</li>\n",
    "        <ul>\n",
    "            <li>It contains the proper nouns (not part of the english dictionary, checked using .isupper) </li>\n",
    "            <li>The reason for this is so that the proper nouns would not be parsed with the tagalog stemmer </li>\n",
    "        </ul>\n",
    "    <li>```tagalog``` parsed using the TagalogStemmer. </li>   \n",
    "        <ul> \n",
    "            <li>```tagalog_str``` will serve as input to the TagalogStemmer. </li>\n",
    "            <li>Then the TagalogStemmer will output a list of the root words. </li>\n",
    "        </ul>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiate bag of words\n",
    "bows = []\n",
    "\n",
    "for i in range(0, len(messages)):\n",
    "\tenglish = []\n",
    "\tproper = []\n",
    "\ttagalog_str = ''\n",
    "\n",
    "\t#For each message tokenize the words, \n",
    "\t#Check if it is a stop word\n",
    "\t#If it is not, append to respective list\n",
    "\tfor word in nlp(messages[i]):\n",
    "\t\tif len(word.text) > 2 and not word.is_stop:\n",
    "\t\t\tif word.text in nlp.vocab:\n",
    "\t\t\t\tenglish.append(word.lemma_)\n",
    "\t\t\telif word.text[0].isupper():\n",
    "\t\t\t\tproper.append(word.lemma_)\n",
    "\t\t\telse:\n",
    "\t\t\t\ttagalog_str += word.text + ' '\n",
    "\n",
    "\ttry:\n",
    "\t\ttagalog = TglStemmer.stemmer('2', tagalog_str, '2')[1]\n",
    "\texcept:\n",
    "\t\t#Skip if tagalog_str is empty\n",
    "\t\tcontinue\n",
    "\t\n",
    "\t#Append the list of all the clean words to the bag of words\n",
    "\tbows.append(english + proper + tagalog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample parsed entries in the bag of words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mall', 'sobrang', 'gaisano', 'panoorin', 'nakakaawa', 'ito', 'sapit', 'batang', 'ipit', 'liwa', 'kamay', 'escalatod', 'buo', 'yari']\n"
     ]
    }
   ],
   "source": [
    "print(bows[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['top', 'story', 'miss', 'the', 'philippines', 'web', 'mass', 'right', 'filipinos', 'access', 'internet', 'albeit', 'exactly', 'trouble', 'free', 'give', 'dismal', 'download', 'speed', 'scant', 'provision', 'nevertheless', 'represent', 'country', 'people', 'glance', 'look', 'impressive', 'lot', 'netizen', 'place', 'but', 'figure', 'big', 'proportion', 'country', 'internet', 'absent', 'whopping', 'population', 'fact', 'that', 'mean', 'non', 'filipinos', 'large', 'and', 'lot', 'place', 'let', 'perspective', 'big', 'combine', 'population', 'south', 'africa', 'city', 'chicago', 'wifi', 'netizen']\n"
     ]
    }
   ],
   "source": [
    "print(bows[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________________________________________\n",
    "## Step 6: Finally, save the bag of words to a file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To json object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('bows.json', 'w') as f:\n",
    "    json.dump(bows, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To text file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "bows_text = []\n",
    "for i in range(0,len(bows)):\n",
    "    bows_text.append(' '.join(b for b in bows[i]))\n",
    "    \n",
    "bows_text = ' '.join(bow for bow in bows_text)\n",
    "\n",
    "with open('bows.txt', 'w') as f:\n",
    "    f.write(bows_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
