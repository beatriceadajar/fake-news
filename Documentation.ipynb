{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fakebook\n",
    "### by Bea Adajar, Gab Barbudo, and Irene Bermejo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem\n",
    "\n",
    "Recently, the propagation of fake news through social media has become a rampant issue all around the world. In the Philippines, it manifests itself through the spread of unverified reports through various Facebook pages and users, even from popular personalities.\n",
    "\n",
    "In this day and age, it is therefore important to be aware that they are being spread and also to know what characteristics to look out for in spotting fake news articles and pages. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research Questions:\n",
    "\n",
    "To answer the problem, we have come up with the following research questions:\n",
    "\n",
    "<ol>\n",
    "    <li> What are the top fake news articles of 2017 published on Facebook, based on the number of shares and reactions? </li>\n",
    "    <li>What are the most common topics of these fake news?</li>\n",
    "    <li>How do people respond to (react, comment, or share) these fake news?</li>\n",
    "    <li>Are there common trends and behaviours by Facebook pages that propagate fake news?</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Dataset\n",
    "\n",
    "## I. Description\n",
    "\n",
    "The dataset will consist of fake news articles published on Facebook in 2017. These posts were obtained through Facebook’s Graph API. \n",
    "\n",
    "## II. Columns \n",
    "\n",
    "+ ```(str) name```: Name of page who originally created the post; only appears when post is shared by current page\n",
    "+ ```(datetime) created_time```: Time post was created\n",
    "+ ```(str) message```:  Message of the post\n",
    "+ ```(int) like```: Number of like reactions for the post\n",
    "+ ```(int) love```: Number of love reactions for the post\n",
    "+ ```(int) wow```: Number of wow reactions for the post\n",
    "+ ```(int) haha```: Number of haha reactions for the post\n",
    "+ ```(int) sad```: Number of sad reactions for the post\n",
    "+ ```(int) angry```: Number of angry reactions for the post\n",
    "+ ```(int) thankful```: Number of thankful reactions for the post\n",
    "+ ```(int) total_reacts```: Total number of reactions for the post\n",
    "+ ```(int) comments```: Number of comments on the post\n",
    "+ ```(int) shares```: Number of shares of the post\n",
    "+ ```(int) id```: The unique identifier of a Facebook post\n",
    "+ ```(int) page_id```: The unique identifier of the Facebook page\n",
    "\n",
    "## III. Scope of the dataset\n",
    "\n",
    "The scope of the dataset covers all 2017 posts from 11 Facebook pages \"carrying fake or unverified contents,\" based on the CBCP Pastoral Guidelines on the Use of Social Media issued on January 30. The said pages are also included in the [CBCP’s list of fake news sites](https://www.rappler.com/nation/173832-cbcp-list-websites-fake-news):\n",
    "\n",
    "+ ClassifiedTrends\n",
    "+ i.am.filipino\n",
    "+ DuterteNewsInfo\n",
    "+ FilipiNewsPH\n",
    "+ benign0\n",
    "+ NetizenOfficialPH\n",
    "+ okd2ads\n",
    "+ PinoyViralNewsPH\n",
    "+ PinoyWorld.co\n",
    "+ PublicTrendingOfficial\n",
    "+ TheVolatilian\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation: FB Fake News Scraper (via Graph API)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Data Gathering\n",
    "\n",
    "First, we gathered the data using Facebook's Graph API, which is used for querying information from Facebook users and groups. These are the following steps we've taken in obtaining the necessary information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import datetime\n",
    "import json\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import urllib.request as urllib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to use the API, an access token is needed. We used an app access token, which should not be shared with anyone; thus, the value of ```access_token``` indicated below was removed for security purposes.\n",
    "\n",
    "```page_ids``` is an array containing the page IDs of all the Facebook pages to be scraped. These sites were selected from <a href='https://www.rappler.com/nation/173832-cbcp-list-websites-fake-news'>CBCP's list fake news sites</a>, and filtered to include only those with Facebook pages that could be accessed via the Graph API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "access_token = '######'    # do not share access token with anyone!!\n",
    "page_ids = ['ClassifiedTrends','i.am.filipino','Dutertenewsinfo','FilipiNewsPH','benign0','NetizenPHOfficial','okd2ads','pinoyviralnewsph','PinoyWorld.co','PublicTrendingOfficial','TheVolatilian']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we defined the functions used for scraping the necessary data.\n",
    "\n",
    "```request_until_succeed``` is a helper function that catches the HTTP Error 400. This is necessary because when large amounts of data are being scraped at a time, this error tends to arise and interrupt the scraper. The helper function catches the errors, pauses for 5 seconds, then tries again, until the URL is successfully opened.\n",
    "\n",
    "```get_feed_data``` is the main function. Given a page ID and access token, it returns the following data:\n",
    "+ ```name```: Name of page who originally created the post; only appears when post is shared by current page\n",
    "+ ```created_time```: Time post was created\n",
    "+ ```message```:  Message of the post\n",
    "+ ```like```: Number of like reactions for the post\n",
    "+ ```love```: Number of love reactions for the post\n",
    "+ ```wow```: Number of wow reactions for the post\n",
    "+ ```haha```: Number of haha reactions for the post\n",
    "+ ```sad```: Number of sad reactions for the post\n",
    "+ ```angry```: Number of angry reactions for the post\n",
    "+ ```thankful```: Number of thankful reactions for the post\n",
    "+ ```total_reacts```: Total number of reactions for the post\n",
    "+ ```comments```: Number of comments on the post\n",
    "+ ```shares```: Number of shares of the post\n",
    "+ ```id```: The unique identifier of a Facebook post\n",
    "+ ```page_id```: The unique identifier of the Facebook page\n",
    "\n",
    "Only posts from 2017 are returned.\n",
    "\n",
    "For now, we've excluded scraping for the names of users and pages who have shared a post. Currently, the node for that request in Graph API is unreliable, so we may do it only once we've determined the top fake news posts.\n",
    "\n",
    "Note that some cleaning is required for the desired formatted output. Here are the functions and the returned data before cleaning and formatting is implemented:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of posts scraped: 278\n",
      "{\n",
      "    \"name\": \"Poor Old Man Accidentally Scratches Luxurious Car, Leaves Note That Touches Everyone’s Heart\",\n",
      "    \"created_time\": \"2017-12-24T20:29:19+0000\",\n",
      "    \"like\": {\n",
      "        \"data\": [],\n",
      "        \"summary\": {\n",
      "            \"total_count\": 52\n",
      "        }\n",
      "    },\n",
      "    \"love\": {\n",
      "        \"data\": [],\n",
      "        \"summary\": {\n",
      "            \"total_count\": 2\n",
      "        }\n",
      "    },\n",
      "    \"wow\": {\n",
      "        \"data\": [],\n",
      "        \"summary\": {\n",
      "            \"total_count\": 0\n",
      "        }\n",
      "    },\n",
      "    \"haha\": {\n",
      "        \"data\": [],\n",
      "        \"summary\": {\n",
      "            \"total_count\": 0\n",
      "        }\n",
      "    },\n",
      "    \"sad\": {\n",
      "        \"data\": [],\n",
      "        \"summary\": {\n",
      "            \"total_count\": 2\n",
      "        }\n",
      "    },\n",
      "    \"angry\": {\n",
      "        \"data\": [],\n",
      "        \"summary\": {\n",
      "            \"total_count\": 0\n",
      "        }\n",
      "    },\n",
      "    \"thankful\": {\n",
      "        \"data\": [],\n",
      "        \"summary\": {\n",
      "            \"total_count\": 0\n",
      "        }\n",
      "    },\n",
      "    \"total_reacts\": {\n",
      "        \"data\": [],\n",
      "        \"summary\": {\n",
      "            \"total_count\": 56\n",
      "        }\n",
      "    },\n",
      "    \"comments\": {\n",
      "        \"data\": [],\n",
      "        \"summary\": {\n",
      "            \"order\": \"ranked\",\n",
      "            \"total_count\": 1,\n",
      "            \"can_comment\": false\n",
      "        }\n",
      "    },\n",
      "    \"shares\": {\n",
      "        \"count\": 4\n",
      "    },\n",
      "    \"id\": \"979290192127575_1645818285474759\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# helper function to catch HTTP Error 400 (Internal Error)\n",
    "def request_until_succeed(url):\n",
    "    success = False\n",
    "    while not success:\n",
    "        try:\n",
    "            response = urllib.urlopen(url)\n",
    "            \n",
    "            if response.getcode() == 200:\n",
    "                success = True\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            time.sleep(5)\n",
    "            print('Error for URL %s: %s' % (url, datetime.datetime.now()))\n",
    "        \n",
    "    return response.read()\n",
    "\n",
    "\n",
    "# returns information about posts on a page\n",
    "def get_feed_data(page_id, access_token):\n",
    "    # create url\n",
    "    base = 'https://graph.facebook.com/v2.12'\n",
    "    node = '/' + page_id + '/feed'\n",
    "    params = '/?fields=name,created_time,message,reactions.type(LIKE).limit(0).summary(total_count).as(like),reactions.type(LOVE).limit(0).summary(total_count).as(love),reactions.type(WOW).limit(0).summary(total_count).as(wow),reactions.type(HAHA).limit(0).summary(total_count).as(haha),reactions.type(SAD).limit(0).summary(total_count).as(sad),reactions.type(ANGRY).limit(0).summary(total_count).as(angry),reactions.type(THANKFUL).limit(0).summary(total_count).as(thankful),reactions.limit(0).summary(total_count).as(total_reacts),comments.limit(0).summary(true),shares&access_token=%s' % (access_token)\n",
    "    url = base + node + params\n",
    "    \n",
    "    # retrieve data\n",
    "    feed_data = json.loads(request_until_succeed(url))\n",
    "    data = []\n",
    "    \n",
    "    # keep paginating until all 2017 posts have been scraped\n",
    "    # note: some posts from 2016 may be included as well\n",
    "    while feed_data['data'][0]['created_time'][:4]!='2016':\n",
    "        try:\n",
    "            data += feed_data['data']\n",
    "            feed_data = requests.get(feed_data['paging']['next']).json()\n",
    "        except KeyError:\n",
    "            break\n",
    "    \n",
    "    # final filtering for 2017 posts only\n",
    "    data = [data[x] for x in range(len(data)) if data[x]['created_time'][:4]=='2017']\n",
    "\n",
    "    print('Number of posts scraped:', len(data))\n",
    "    return data\n",
    "\n",
    "\n",
    "# only print the first result\n",
    "print(json.dumps(get_feed_data(page_ids[0], access_token)[0], indent=4, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "A lot of the data is unnecessarily nested, and not all fields are required. Thus, we added in code to remove what is not needed and to fix the format of the output.\n",
    "\n",
    "Below are the new functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of posts scraped: 302\n",
      "{\n",
      "    \"name\": \"Poor Old Man Accidentally Scratches Luxurious Car, Leaves Note That Touches Everyone’s Heart\",\n",
      "    \"created_time\": \"2017-12-24T20:29:19+0000\",\n",
      "    \"like\": 52,\n",
      "    \"love\": 2,\n",
      "    \"wow\": 0,\n",
      "    \"haha\": 0,\n",
      "    \"sad\": 2,\n",
      "    \"angry\": 0,\n",
      "    \"thankful\": 0,\n",
      "    \"total_reacts\": 56,\n",
      "    \"comments\": 1,\n",
      "    \"shares\": 4,\n",
      "    \"id\": \"979290192127575_1645818285474759\",\n",
      "    \"page_id\": \"ClassifiedTrends\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# helper function to catch HTTP Error 400 (Internal Error)\n",
    "def request_until_succeed(url):\n",
    "    success = False\n",
    "    while not success:\n",
    "        try:\n",
    "            response = urllib.urlopen(url)\n",
    "            \n",
    "            if response.getcode() == 200:\n",
    "                success = True\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            time.sleep(5)\n",
    "            print('Error for URL %s: %s' % (url, datetime.datetime.now()))\n",
    "        \n",
    "    return response.read()\n",
    "\n",
    "\n",
    "# returns information about posts on a page\n",
    "def get_feed_data(page_id, access_token):\n",
    "    # create url\n",
    "    base = 'https://graph.facebook.com/v2.12'\n",
    "    node = '/' + page_id + '/feed'\n",
    "    params = '/?fields=name,created_time,message,reactions.type(LIKE).limit(0).summary(total_count).as(like),reactions.type(LOVE).limit(0).summary(total_count).as(love),reactions.type(WOW).limit(0).summary(total_count).as(wow),reactions.type(HAHA).limit(0).summary(total_count).as(haha),reactions.type(SAD).limit(0).summary(total_count).as(sad),reactions.type(ANGRY).limit(0).summary(total_count).as(angry),reactions.type(THANKFUL).limit(0).summary(total_count).as(thankful),reactions.limit(0).summary(total_count).as(total_reacts),comments.limit(0).summary(true),shares&access_token=%s' % (access_token)\n",
    "    url = base + node + params\n",
    "    \n",
    "    # retrieve data\n",
    "    feed_data = json.loads(request_until_succeed(url))\n",
    "    data = []\n",
    "    data += feed_data['data']\n",
    "    \n",
    "    # keep paginating until all 2017 posts have been scraped\n",
    "    while feed_data['data'][0]['created_time'][:4]!='2016':\n",
    "        try:\n",
    "            data += feed_data['data']\n",
    "            feed_data = requests.get(feed_data['paging']['next']).json()\n",
    "        except KeyError:\n",
    "            break\n",
    "    \n",
    "    # final filtering for 2017 posts\n",
    "    data = [data[x] for x in range(len(data)) if data[x]['created_time'][:4]=='2017']\n",
    "    \n",
    "    # ADDITIONAL CODE NOT IN PREVIOUS CELL\n",
    "    # fix output format\n",
    "    for x in data:\n",
    "        try:\n",
    "            # add page_id\n",
    "            x['page_id'] = page_id\n",
    "            # remove unnecessary data\n",
    "            x['like'] = x['like']['summary']['total_count']\n",
    "            x['love'] = x['love']['summary']['total_count']\n",
    "            x['wow'] = x['wow']['summary']['total_count']\n",
    "            x['haha'] = x['haha']['summary']['total_count']\n",
    "            x['sad'] = x['sad']['summary']['total_count']\n",
    "            x['angry'] = x['angry']['summary']['total_count']\n",
    "            x['thankful'] = x['thankful']['summary']['total_count']\n",
    "            x['total_reacts'] = x['total_reacts']['summary']['total_count']\n",
    "            x['comments'] = x['comments']['summary']['total_count']\n",
    "            x['shares'] = x['shares']['count']\n",
    "        except Exception:\n",
    "            # if attribute does not exist, do nothing\n",
    "            pass\n",
    "    \n",
    "    print('Number of posts scraped:', len(data))\n",
    "    return data\n",
    "\n",
    "\n",
    "# only print the first result\n",
    "print(json.dumps(get_feed_data(page_ids[0], access_token)[0], indent=4, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then applied the function to all the Facebook pages in the ```page_ids``` array. Afterwards, we dumped the data into a json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                 | 0/11 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of posts scraped: 302\n",
      "Scraped: ClassifiedTrends\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  9%|█████▏                                                   | 1/11 [00:17<02:52, 17.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of posts scraped: 981\n",
      "Scraped: i.am.filipino\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 18%|██████████▏                                             | 2/11 [05:35<25:11, 167.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of posts scraped: 115\n",
      "Scraped: Dutertenewsinfo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 27%|███████████████▎                                        | 3/11 [05:48<15:28, 116.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of posts scraped: 340\n",
      "Scraped: FilipiNewsPH\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 36%|████████████████████▋                                    | 4/11 [06:19<11:04, 94.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of posts scraped: 615\n",
      "Scraped: benign0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 45%|█████████████████████████▉                               | 5/11 [07:38<09:10, 91.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of posts scraped: 570\n",
      "Scraped: NetizenPHOfficial\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 55%|███████████████████████████████                          | 6/11 [09:48<08:10, 98.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of posts scraped: 607\n",
      "Scraped: okd2ads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 64%|████████████████████████████████████▎                    | 7/11 [10:54<06:13, 93.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of posts scraped: 592\n",
      "Scraped: pinoyviralnewsph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 73%|█████████████████████████████████████████▍               | 8/11 [12:12<04:34, 91.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of posts scraped: 57\n",
      "Scraped: PinoyWorld.co\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 82%|██████████████████████████████████████████████▋          | 9/11 [12:18<02:44, 82.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of posts scraped: 34\n",
      "Scraped: PublicTrendingOfficial\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 91%|██████████████████████████████████████████████████▉     | 10/11 [12:22<01:14, 74.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of posts scraped: 409\n",
      "Scraped: TheVolatilian\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████| 11/11 [13:01<00:00, 71.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved data to JSON file.\n"
     ]
    }
   ],
   "source": [
    "fb_data = []\n",
    "\n",
    "for i in tqdm(range(len(page_ids))):\n",
    "    try:\n",
    "        page = page_ids[i]\n",
    "        temp = get_feed_data(page, access_token)\n",
    "\n",
    "        for x in temp:\n",
    "            x['page_id'] = page\n",
    "\n",
    "        print('Scraped:', page)\n",
    "        fb_data += temp\n",
    "    except Exception as e:\n",
    "        print('Error:', e)\n",
    "\n",
    "with open('data.json', 'w', encoding='utf-8') as outfile:\n",
    "    json.dump(fb_data, outfile, ensure_ascii=False, indent=4)\n",
    "    print('Successfully saved data to JSON file.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a preview of the JSON file as a DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>created_time</th>\n",
       "      <th>like</th>\n",
       "      <th>love</th>\n",
       "      <th>wow</th>\n",
       "      <th>haha</th>\n",
       "      <th>sad</th>\n",
       "      <th>angry</th>\n",
       "      <th>thankful</th>\n",
       "      <th>total_reacts</th>\n",
       "      <th>comments</th>\n",
       "      <th>shares</th>\n",
       "      <th>post_id</th>\n",
       "      <th>page_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4617</th>\n",
       "      <td>Investment advice: ignore the doomsayers - The...</td>\n",
       "      <td>2017-01-01 23:07:42</td>\n",
       "      <td>439</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>461</td>\n",
       "      <td>10</td>\n",
       "      <td>69.0</td>\n",
       "      <td>2.935174e+29</td>\n",
       "      <td>TheVolatilian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4618</th>\n",
       "      <td>Big actors waiting in the wings - The Volatilian™</td>\n",
       "      <td>2017-01-01 23:05:09</td>\n",
       "      <td>558</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>563</td>\n",
       "      <td>14</td>\n",
       "      <td>27.0</td>\n",
       "      <td>2.935174e+29</td>\n",
       "      <td>TheVolatilian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4619</th>\n",
       "      <td>The Philippines’ Web-less masses - The Volatil...</td>\n",
       "      <td>2017-01-01 23:03:06</td>\n",
       "      <td>522</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>529</td>\n",
       "      <td>8</td>\n",
       "      <td>23.0</td>\n",
       "      <td>2.935174e+29</td>\n",
       "      <td>TheVolatilian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4620</th>\n",
       "      <td>Philippine trade opportunities from Brexit - T...</td>\n",
       "      <td>2017-01-01 23:00:34</td>\n",
       "      <td>426</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>450</td>\n",
       "      <td>5</td>\n",
       "      <td>40.0</td>\n",
       "      <td>2.935174e+29</td>\n",
       "      <td>TheVolatilian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4621</th>\n",
       "      <td>Plan to get ship-shape and competitive - The V...</td>\n",
       "      <td>2017-01-01 22:57:38</td>\n",
       "      <td>536</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>544</td>\n",
       "      <td>6</td>\n",
       "      <td>23.0</td>\n",
       "      <td>2.935174e+29</td>\n",
       "      <td>TheVolatilian</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   name        created_time  \\\n",
       "4617  Investment advice: ignore the doomsayers - The... 2017-01-01 23:07:42   \n",
       "4618  Big actors waiting in the wings - The Volatilian™ 2017-01-01 23:05:09   \n",
       "4619  The Philippines’ Web-less masses - The Volatil... 2017-01-01 23:03:06   \n",
       "4620  Philippine trade opportunities from Brexit - T... 2017-01-01 23:00:34   \n",
       "4621  Plan to get ship-shape and competitive - The V... 2017-01-01 22:57:38   \n",
       "\n",
       "      like  love  wow  haha  sad  angry  thankful  total_reacts  comments  \\\n",
       "4617   439    15    3     4    0      0         0           461        10   \n",
       "4618   558     3    2     0    0      0         0           563        14   \n",
       "4619   522     3    2     1    1      0         0           529         8   \n",
       "4620   426    11   13     0    0      0         0           450         5   \n",
       "4621   536     4    2     2    0      0         0           544         6   \n",
       "\n",
       "      shares       post_id        page_id  \n",
       "4617    69.0  2.935174e+29  TheVolatilian  \n",
       "4618    27.0  2.935174e+29  TheVolatilian  \n",
       "4619    23.0  2.935174e+29  TheVolatilian  \n",
       "4620    40.0  2.935174e+29  TheVolatilian  \n",
       "4621    23.0  2.935174e+29  TheVolatilian  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json('data.json', encoding='utf-8')\n",
    "df = df[['name','created_time','like','love','wow','haha','sad','angry','thankful','total_reacts','comments','shares','id','page_id']]\n",
    "df = df.rename(columns={'id':'post_id'})\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description of Data Visualization Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Word Bar Graphs and Pie Charts\n",
    "\n",
    "In order to count the words, we want to get the base form of the words and remove unnecessary stop words.\n",
    "\n",
    "### String Cleaning and Lemmatization\n",
    "\n",
    "<ol>\n",
    "<li>For each message in the data, urls, special characters, extra white space and numbers are removed from the message. </li>\n",
    "<br />\n",
    "<li> Then spaCy library is used for preprocessing and lemmatizing the English words. Since Tagalog is currently not supported in NLP libraries in python, the Tagalog Words Stemmer (https://github.com/crlwingen/TagalogStemmerPython/blob/master/TglStemmer.py) is used to process the Tagalog words. This removes the affixes of the Tagalog word and returns the root word. Since the Tagalog Stemmer only returns the root of tagalog words, the Tagalog stop words are added to spaCy's dictionary.\n",
    "<br />\n",
    "<br />\n",
    "Tagalog stop words are from https://github.com/stopwords-iso/stopwords-tl/blob/master/stopwords-tl.json\n",
    "<br />\n",
    "<br />\n",
    "</li>\n",
    "\n",
    "<li>\n",
    "Map each word to its respective list. Three separate lists are used because each list will be cleaned differently. This is to consider that the language may be Tagalog, Taglish, or English. *Other Filipino languages (e.g Bisaya) will be included in the tagalog list)*\n",
    "\n",
    "Since word count is the only concern, the order of these messages do not matter.\n",
    "\n",
    "<br \\>\n",
    "\n",
    "Lists:\n",
    "<ul>\n",
    "    <li> ```english``` parsed using the standard lemma_ of spaCy's nlp. </li>\n",
    "    <br />\n",
    "    <li>```proper``` parses also using the standard lemma_of spaCy's nlp.</li>\n",
    "    <br />\n",
    "        <ul>\n",
    "            <li>It contains the proper nouns (not part of the english dictionary, checked using .isupper) </li>\n",
    "            <li>The reason for this is so that the proper nouns would not be parsed with the tagalog stemmer </li>\n",
    "        </ul>\n",
    "    <br />\n",
    "    <li>```tagalog``` parsed using the TagalogStemmer. </li>   \n",
    "        <ul> \n",
    "            <li> ```tagalog_str``` will serve as input to the TagalogStemmer. </li>\n",
    "            <li>Then the TagalogStemmer will output a list of the root words. </li>\n",
    "        </ul>\n",
    "</ul>\n",
    "\n",
    "</li>\n",
    "\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Instantiate bag of words\n",
    "bows = []\n",
    "\n",
    "for i in range(0, len(messages)):\n",
    "    english = []\n",
    "    proper = []\n",
    "    tagalog_str = '' # This is a string because stemmer requires string input\n",
    "\n",
    "    # For each message tokenize the words, \n",
    "    # Check if it is a stop word\n",
    "    # If it is not, append to respective list\n",
    "    for word in nlp(messages[i]):\n",
    "        if len(word.text) > 2 and not word.is_stop:\n",
    "            if word.text in nlp.vocab:\n",
    "                english.append(word.lemma_)\n",
    "            elif word.text[0].isupper():\n",
    "                proper.append(word.lemma_)\n",
    "            else:\n",
    "                tagalog_str += word.text + ' ' \n",
    "\n",
    "    try:\n",
    "        tagalog = TglStemmer.stemmer('2', tagalog_str, '2')[1]\n",
    "    except:\n",
    "        # Skip if tagalog_str is empty\n",
    "        continue\n",
    "\n",
    "    # Append the list of all the clean words to the bag of words\n",
    "    bows.append(english + proper + tagalog)\n",
    "    \n",
    "    # bows is then stored to a json file and text file, bows.json and bows.txt respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bar Graph (Frequency of Word)\n",
    "\n",
    "In order to get the top 10 words:  \n",
    "- ```cat bows.txt | tr ' ' '\\n' | sort | uniq -c | sort > freq.txt``` command is run on the terminal.\n",
    "\n",
    "This generated the final list of top 10 words/category (joining similar words that refer to the same person/thing into one category):\n",
    "\n",
    "- word_list = [['medium'], ['share'], ['filipinos', 'filipino'], ['country'], ['illegal'], ['robredo', 'leni'], ['jueteng', 'uete'], ['drugs', 'drug', 'droga'], ['pilipinas','philippines', 'philippine', 'philippines'],  ['duterte', 'president', 'rodrigo']]\n",
    "\n",
    "- word_occurences = [178, 181, 184, 187, 187, 187, 304, 354, 695, 971]\n",
    "\n",
    "\n",
    "\n",
    "### Pie Chart (No. of Occurences)\n",
    "\n",
    "Using the top 10 words/categories from the word_list, for each word/category count the number of messages it appears in the bows.json.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for bow in bows:\n",
    "    for i in range(0, len(word_list)):\n",
    "        for word in word_list[i]:\n",
    "            if word in bow:\n",
    "                word_count[i] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This resulted to the final word_count = [101, 170, 155, 132, 34, 141, 16, 97, 733, 705]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Scatter Plots\n",
    "\n",
    "### Scatter Plot (All Articles)\n",
    "\n",
    "The scatter plot contains all the fake news articles from the ten chosen Facebook pages and the number of reactions (x axis) and shares (y axis) of each article. This is to show which topics have the highest reach.\n",
    "\n",
    "### Bubble Chart (By Page)\n",
    "\n",
    "The bubble chart contains the total number of reactions (x axis), shares (y axis), and articles (radius) each page has to show which page has the highest reach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Reactions/Shares/Comments Bar Graph\n",
    "\n",
    "This bar graph displays the top ten posts based on either number of reactions, shares, or comments. There is a dropdown that allows the user to toggle between criteria to see whether or not the posts that rank highest in one criterion are the same in other criteria.\n",
    "\n",
    "The x-axis previews the title of the post while the y-axis is the number of reactions/shares/comments. Each bar has a tooltip that displays the complete title of the post, the content of the post's caption or message, and the number of reactions/shares/comments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Trend Graph\n",
    "\n",
    "The Trend graph contains all posts by date and score/reach per page in order to see if there is a certain trend/behaviour on how the page posts.\n",
    "\n",
    "For each post, the score/reach points are calculated with weights: likes * 0.25 + (total_reacts - likes) * 0.5 + comments * 0.75 + shares. Each is pushed into an array to be plotted in the graph chronologically. \n",
    "\n",
    "``` javascript\n",
    "var arr = [];\n",
    "\n",
    "var convertPoints = function(data, label1, label2) {\n",
    "    var obj = Object.values(data).map(function(c) {\n",
    "    var score = (c.like * 0.25 + (c.total_reacts - c.like) * 0.5 + c.comments * 0.75 + c.shares * 1) || 0;\n",
    "                                                   \n",
    "        return {[label1]: moment(c.created_time).format('YYYY-MM-DD HH:mm:ss').toString(),[label2]: score, \n",
    "                title: c.name};\n",
    "    });\n",
    "    Object.keys(obj).forEach(function(item) {\n",
    "    arr.push(obj[item]);\n",
    "    })\n",
    "\n",
    "convertPoints(ClassifiedTrends, \"date1\", \"y1\");\n",
    "convertPoints(IAmFilipino, \"date2\", \"y2\");\n",
    "convertPoints(DuterteNewsInfo, \"date3\", \"y3\");\n",
    "convertPoints(FilipiNewsPH, \"date4\", \"y4\");\n",
    "convertPoints(Benigno, \"date5\", \"y5\");\n",
    "convertPoints(NetizenPHOfficial, \"date6\", \"y6\");\n",
    "convertPoints(Okd2Ads, \"date7\", \"y7\");\n",
    "convertPoints(PinoyViralNewsPH, \"date8\", \"y8\");\n",
    "convertPoints(PinoyWorld, \"date9\", \"y9\");\n",
    "convertPoints(PublicTrendingOfficial, \"date10\", \"y10\");\n",
    "convertPoints(TheVolatilian, \"date11\", \"y11\");\n",
    "\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Findings\n",
    "\n",
    "- The most common fake news topics and commonly used words are related to politics. More specifically, the top frequently-used words are related to politics and crimes (\"Duterte,\" \"president,\" \"Rodrigo, \"jueteng,\" \"drugs/droga\").\n",
    "- However, the posts with the highest reach are articles which have nothing to do with politics. In fact, they are not actually classified as \"fake news\" but more appropriately \"clickbait.\"\n",
    "- The topics that have the most number of shares and reactions are clickbait. Meanwhile, the topics that have the most number of comments are related to politics.\n",
    "- The common trend of pages is there is a majority of posts with steady low reach points and very few yet dominant spikes of high reach points. The ones with higher reach points are the clickbait articles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Critical Evaluation\n",
    "\n",
    "- These pages could be a strategy of propaganda. Clickbaits are used in order to gain reach and followers which gives them an avenue to flood post on political agendas. This possibility is supported by the fact that there is higher interaction with articles that are considered clickbait, but more articles about politics.\n",
    "- Although clickbait articles garner more attention through reactions and shares, the political posts generate more discussion as there are more comments. This means that although the reach may not be as high in terms of number of reactions and shares, more people are vocal about their thoughts regarding political issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problems, Issues, and Limitations\n",
    "\n",
    "+ Limited to handpicked pages. It is difficult to personally specify fake news pages, which would involve a lot of searching through Facebook as well as an establishment of a concrete criteria.\n",
    "+ Topic is limited to the most commonly used words.\n",
    "+ Data that could be acquired was also limited by what the Graph API could retrieve. Scraping Facebook for data through any other means is not allowed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
